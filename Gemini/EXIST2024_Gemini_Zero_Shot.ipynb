{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWzzODvtJkxa"
      },
      "source": [
        "## 1. Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8jC360DJkxc"
      },
      "outputs": [],
      "source": [
        "!pip install PyEvALL\n",
        "from pyevall.evaluation import PyEvALLEvaluation\n",
        "from pyevall.metrics.metricfactory import MetricFactory\n",
        "from pyevall.utils.utils import PyEvALLUtils\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdlI6QPTJkxd"
      },
      "source": [
        "## 2. JSON manipulation\n",
        "This section contains the functions used to process the dataset and store the answers given by the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ_s_lLSJkxd"
      },
      "outputs": [],
      "source": [
        "def json_df(json_location, dataframe_name):\n",
        "    \"\"\"\n",
        "    Load a JSON file and convert it into a pandas DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    json_location (str): The file path of the JSON file.\n",
        "    dataframe_name (str): The name of the resulting DataFrame.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: The DataFrame created from the JSON data.\n",
        "    \"\"\"\n",
        "    with open(json_location, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    dataframe_name = pd.DataFrame.from_dict(data, orient='index')\n",
        "    return dataframe_name\n",
        "\n",
        "def load_prompt(file_path):\n",
        "    \"\"\"\n",
        "    Load the prompt from a file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the file containing the prompt.\n",
        "\n",
        "    Returns:\n",
        "        str: The loaded prompt.\n",
        "\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        prompt = file.readline().strip()\n",
        "    return prompt\n",
        "\n",
        "def save_responses(responses_dict, output_dir='.', base_filename='NO_FILE_NAME_RMIT', increment_filename=True):\n",
        "    \"\"\"\n",
        "    Save responses to a JSON file.\n",
        "\n",
        "    Args:\n",
        "        responses_dict (dict): A dictionary containing the responses to be saved.\n",
        "        output_dir (str, optional): The directory where the JSON file will be saved. Defaults to '.'.\n",
        "        base_filename (str, optional): The base filename for the JSON file. Defaults to 'NO_FILE_NAME_RMIT'.\n",
        "        increment_filename (bool, optional): Whether to increment the filename if it already exists. Defaults to True.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If responses_dict is not a dictionary.\n",
        "\n",
        "    \"\"\"\n",
        "    if not isinstance(responses_dict, dict):\n",
        "        raise ValueError(\"responses_dict must be a dictionary.\")\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    filename = f\"{base_filename}.json\" if not increment_filename else get_next_filename(output_dir, base_filename)\n",
        "    full_path = os.path.join(output_dir, filename)\n",
        "\n",
        "    # Convert dictionary to list for JSON dumping\n",
        "    responses_list = list(responses_dict.values())\n",
        "\n",
        "    try:\n",
        "        with open(full_path, 'w') as file:\n",
        "            json.dump(responses_list, file, indent=4)\n",
        "        print(f\"Data saved to {filename}\")\n",
        "    except IOError as e:\n",
        "        print(f\"Failed to save data: {e}\")\n",
        "\n",
        "def get_next_filename(output_dir, base_filename):\n",
        "    \"\"\"\n",
        "    Get the next available filename for a given base filename in the specified output directory.\n",
        "\n",
        "    Args:\n",
        "        output_dir (str): The directory where the files are stored.\n",
        "        base_filename (str): The base filename to be used.\n",
        "\n",
        "    Returns:\n",
        "        str: The next available filename in the format \"{base_filename}_{number}.json\".\n",
        "\n",
        "    \"\"\"\n",
        "    pattern = re.compile(rf\"{re.escape(base_filename)}_(\\d+).json\")\n",
        "    max_number = 0\n",
        "    for filename in os.listdir(output_dir):\n",
        "        match = pattern.match(filename)\n",
        "        if match:\n",
        "            number = int(match.group(1))\n",
        "            if number > max_number:\n",
        "                max_number = number\n",
        "    return f\"{base_filename}_{max_number + 1}.json\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zUhX8JZJkxe"
      },
      "source": [
        "## 3. Query the model\n",
        "This section cotains the functions used to submit the tweets to Gemini and get classification answers based on task and evaluation method. Before running, configure a valid API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhnE0pTXbFnR"
      },
      "outputs": [],
      "source": [
        "def get_answer(tweet, eval, prompt, df, idx, task):\n",
        "  \"\"\"\n",
        "    Returns a response from the Gemini API based on the given parameters.\n",
        "\n",
        "    Args:\n",
        "        tweet (str): The tweet for which the response is requested.\n",
        "        api_key (str): The API key for accessing the Gemini API.\n",
        "        eval (str): The evaluation mode ('hard' or 'soft').\n",
        "        prompt (str): The prompt value for the conversation.\n",
        "        df (pandas.DataFrame): The DataFrame containing study levels and gender information.\n",
        "        idx (int): The index of the current row in the DataFrame.\n",
        "        task (int): The task number.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the tweet and the response from the API.\n",
        "\n",
        "    Raises:\n",
        "        Exception: If an error occurs during the API request.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "  delimiter = \"####\"\n",
        "  response_value = \"\"\n",
        "  prompt_value = f\"\"\"\"Sexism is defied as prejudice or discrimination based on sex or gender, especially against women and girls. Although its origin is unclear, the term sexism emerged from the “second-wave” feminism of the 1960s through ’80s and was most likely modeled on the civil rights movement’s term racism (prejudice or discrimination based on race). Sexism can be a belief that one sex is superior to or more valuable than another sex. It imposes limits on what men and boys can and should do and what women and girls can and should do. The concept of sexism was originally formulated to raise consciousness about the oppression of girls and women, although by the early 21st century it had sometimes been expanded to include the oppression of any sex, including men and boys, intersex people, and transgender people. A text is a DIRECT sexist message if the intention is to write a message that is sexist by itself or incites to be sexist. A text is a REPORTED sexist message if the intention is to report and share a sexist situation suffered by a woman or women in first or third person. A text is JUDGMENTAL if the intention is to condemn sexist situations or behaviours. A sexist text is labeled as IDEOLOGICAL-INEQUALITY if it includes messages that discredit the feminist movement or if it includes messages that reject inequality between men and women or if it present men as victims of gender-based oppression. A sexist text is labeled as STEREOTYPING-DOMINANCE if it includes messages that express false ideas about women that suggest they are more suitable or inappropriate for certain tasks, and somehow inferior to men. A sexist text is labeled as OBJECTIFICATION if it includes messages where women are presented as objects apart from their dignity and personal aspects or if it icludes messages that assume or describe certain physical qualities that women must have in order to fulfill traditional gender roles. A sexist text is labeled as SEXUAL-VIOLENCE if it includes messages where sexual suggestions, requests or harassment of a sexual nature (rape or sexual assault) are made. A sexist text is labeled as MISOGYNY-NON-SEXUAL-VIOLENCE if it includes expressions of hatred and violence towards women. You are a robot who detects sexism from text given in the prompt.\"\"\"\n",
        "  column_value = f\"\"\"For each response, consider the perspective of individuals representing the following study levels: {df.study_levels_annotators[idx]} and gender: {df.gender_annotators[idx]}.\"\"\"\n",
        "  if eval == 'hard':\n",
        "      if task == 1:\n",
        "        # Provide an example\n",
        "          response_value = f\"\"\"Give me 1 answer with [NO] or [YES]. If the text is sexist answer [YES]. If the text is not sexist answer [NO]. Example of the syntax of the answer: [YES]\"\"\"\n",
        "      elif task == 2:\n",
        "          response_value = f\"\"\"Give me 1 answer with [NO], [DIRECT], [REPORTED] or [JUDGEMENTAL]. If the text is sexist classify it. If the text is not sexist answer [NO]. Example of the syntax of the answer: [DIRECT]\"\"\"\n",
        "      elif task == 3:\n",
        "          response_value = f\"\"\"Give me a list of 1 to 5 answers separated by commas. If the text is sexist, classify it. If the text is not sexist answer [NO]. This is a multi-label task, so that more than one of the following labels may be assigned to each sexist tweet: [IDEOLOGICAL-INEQUALITY], [STEREOTYPING-DOMINANCE], [OBJECTIFICATION], [SEXUAL-VIOLENCE] or [MISOGYNY-NON-SEXUAL-VIOLENCE]. Example of the syntax of the answer: [OBJECTIFICATION], [SEXUAL-VIOLENCE]\"\"\"\n",
        "\n",
        "  if eval == 'soft':\n",
        "      if task == 1:\n",
        "          response_value = f\"\"\"Give me a list of 6 answers with NO or YES reflecting the perspective of each individual. If an individual judges the text as sexist his or her perspective is [YES]. If an individual judges the text as not sexist his or her perspective is [NO]. Example of the syntax of the answer: [NO], [YES], [NO], [YES], [YES], [YES]\"\"\"\n",
        "      elif task == 2:\n",
        "          response_value = f\"\"\"Give me a list of 6 answers with [NO], [DIRECT], [REPORTED] or [JUDGEMENTAL] reflecting the perspective of each individual and using commas for each answer. If an individual judges the text as sexist his or her perspective is only one label among [DIRECT], [REPORTED] or [JUDGEMENTAL]. If an individual judges the text as not sexist his or her perspective is [NO]. Example of the syntax of the answer: [NO], [DIRECT], [REPORTED], [JUDGEMENTAL], [JUDGEMENTAL], [NO]\"\"\"\n",
        "      elif task == 3:\n",
        "          response_value = f\"\"\"Give me a list of 6 answers with [NO], [IDEOLOGICAL-INEQUALITY], [STEREOTYPING-DOMINANCE], [OBJECTIFICATION], [SEXUAL-VIOLENCE] or [MISOGYNY-NON-SEXUAL-VIOLENCE] reflecting the perspective of each individual and using commas for each answer. If an individual judges the text as sexist his or her perspective is only one label among [IDEOLOGICAL-INEQUALITY], [STEREOTYPING-DOMINANCE], [OBJECTIFICATION], [SEXUAL-VIOLENCE] or [MISOGYNY-NON-SEXUAL-VIOLENCE]. If an individual judges the text as not sexist his or her perspective is [NO]. Example of the syntax of the answer: [NO], [IDEOLOGICAL-INEQUALITY], [STEREOTYPING-DOMINANCE], [OBJECTIFICATION], [SEXUAL-VIOLENCE], [MISOGYNY-NON-SEXUAL-VIOLENCE]\"\"\"\n",
        "\n",
        "  safe = [\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
        "        \"threshold\": \"BLOCK_NONE\",\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "        \"threshold\": \"BLOCK_NONE\",\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "        \"threshold\": \"BLOCK_NONE\",\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "        \"threshold\": \"BLOCK_NONE\",\n",
        "    },\n",
        "  ]\n",
        "\n",
        "  genai.configure(api_key='AIzaSyCviN3hVVCO7a23OV9HR4NXVOnjnoEfEQc')\n",
        "  model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "  answer = model.generate_content(\n",
        "    f\"\"\"{prompt_value} {column_value} {response_value} {delimiter} {tweet} {delimiter}\"\"\",\n",
        "    safety_settings=safe,\n",
        "    generation_config=genai.types.GenerationConfig(\n",
        "        max_output_tokens=None,\n",
        "        temperature=1.0,\n",
        "    )\n",
        "  ).text\n",
        "\n",
        "\n",
        "  res = {\n",
        "      'tweet': tweet,\n",
        "      'response': answer\n",
        "  }\n",
        "\n",
        "  print(\"----\")\n",
        "  print(tweet)\n",
        "  print(answer)\n",
        "  print(\"----\")\n",
        "\n",
        "  return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfxmeasW5WzE"
      },
      "source": [
        "## 4. Normalize answers\n",
        "This section contains the functions used to format the answers properly. For the soft evaluation, it computes the probabilities associated to each label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETRWtYj9Jkxe"
      },
      "outputs": [],
      "source": [
        "def normalize_response(response, eval, task):\n",
        "    \"\"\"\n",
        "    Normalize the response based on the evaluation method and task.\n",
        "\n",
        "    Args:\n",
        "        response (str): The response to be normalized.\n",
        "        eval (str): The evaluation method ('soft' or 'hard').\n",
        "        task (int): The task number (1, 2, or 3).\n",
        "\n",
        "    Returns:\n",
        "        dict or list: If eval is 'soft', returns a dictionary with normalized counts of response categories.\n",
        "                      If eval is 'hard', returns a list of normalized response categories.\n",
        "\n",
        "    \"\"\"\n",
        "    answers = response.replace('[', '').replace(']', '').split(',')\n",
        "    answers = [answer.strip().upper() for answer in answers]  # Clean and normalize case\n",
        "    # Define response categories based on the task and evaluation method\n",
        "    if eval == 'soft':\n",
        "        if task == 1:\n",
        "            counts = {'NO': 0, 'YES': 0}\n",
        "        elif task == 2:\n",
        "            counts = {'NO': 0, 'DIRECT': 0, 'REPORTED': 0, 'JUDGEMENTAL': 0}\n",
        "        elif task == 3:\n",
        "            counts = {'NO': 0, 'IDEOLOGICAL-INEQUALITY': 0, 'STEREOTYPING-DOMINANCE': 0, 'OBJECTIFICATION': 0,'SEXUAL-VIOLENCE':0, 'MISOGYNY-NON-SEXUAL-VIOLENCE':0}\n",
        "        else:\n",
        "            # Return empty dictionary for unspecified tasks or evaluation modes\n",
        "            print(answers)\n",
        "            return {}\n",
        "\n",
        "        # Count responses\n",
        "        for answer in answers:\n",
        "            if answer in counts:\n",
        "                counts[answer] += 1\n",
        "\n",
        "        # Normalize counts by the total number of responses\n",
        "        total = sum(counts.values())\n",
        "        if total > 0:\n",
        "            for key in counts:\n",
        "                counts[key] = counts[key] / total\n",
        "        return counts\n",
        "    else:\n",
        "        return answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAiSI2G86FUx"
      },
      "source": [
        "## 5. Pipeline definition\n",
        "This section creates the pipeline for data ingestion, classification and predictions storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nukRtwLc6AqT"
      },
      "outputs": [],
      "source": [
        "def main(parameters):\n",
        "    \"\"\"\n",
        "    Fetches and processes tweets asynchronously.\n",
        "\n",
        "    Args:\n",
        "        **kwargs: Keyword arguments containing the following parameters:\n",
        "            - dataframe: The dataframe containing the tweets.\n",
        "            - prompt: The prompt for generating responses.\n",
        "            - eval: The evaluation mode for the response.\n",
        "            - task: The task number for processing the tweets.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing two dictionaries:\n",
        "        - responses_dict: A dictionary mapping row IDs to normalized response values.\n",
        "        - raw_responses: A dictionary mapping row IDs to raw response values.\n",
        "    \"\"\"\n",
        "    tweets = parameters['dataframe']['tweet'].to_list()\n",
        "    prompt = parameters['prompt']\n",
        "    eval = parameters['eval']\n",
        "    df = parameters['dataframe']\n",
        "    task = parameters['task']\n",
        "\n",
        "    responses_dict = {}\n",
        "    raw_responses = {}  # Dictionary to store non-normalized responses\n",
        "\n",
        "    for i, tweet in enumerate(tweets):\n",
        "      # Fetch and process each tweet\n",
        "      if task == 1:\n",
        "        response = get_answer(tweet, eval, prompt, df, i, task)\n",
        "        if 'response' in response:\n",
        "          normalized_values = normalize_response(response['response'], eval, task)\n",
        "          row_id = df[df['tweet'] == tweet]['id_EXIST'].values[0]\n",
        "          responses_dict[row_id] = {\n",
        "                        'id': row_id,\n",
        "                        'value': normalized_values,\n",
        "                        'test_case': \"EXIST2024\"\n",
        "                    }\n",
        "          # Store raw response\n",
        "          raw_responses[row_id] = {\n",
        "                        'id': row_id,\n",
        "                        'response': response['response'],\n",
        "                        'test_case': \"EXIST2024\"\n",
        "                    }\n",
        "        else:\n",
        "          print(f\"Error or unexpected format in response for tweet ID {tweet}: {response}\")\n",
        "\n",
        "      if task == 2:\n",
        "        response = get_answer(tweet, eval, prompt, df, i, task)\n",
        "        if 'response' in response:\n",
        "          normalized_values = normalize_response(response['response'], eval, task)\n",
        "          row_id = df[df['tweet'] == tweet]['id_EXIST'].values[0]\n",
        "          responses_dict[row_id] = {\n",
        "                        'id': row_id,\n",
        "                        'value': normalized_values,\n",
        "                        'test_case': \"EXIST2024\"\n",
        "                    }\n",
        "\n",
        "      if task == 3:\n",
        "        response = get_answer(tweet, eval, prompt, df, i, task)\n",
        "        normalized_values = normalize_response(response['response'], eval, task)\n",
        "        row_id = df[df['tweet'] == tweet]['id_EXIST'].values[0]\n",
        "        responses_dict[row_id] = {\n",
        "                        'id': row_id,\n",
        "                        'value': normalized_values,\n",
        "                        'test_case': \"EXIST2024\"\n",
        "                    }\n",
        "\n",
        "      # Sleep to spread requests evenly across the rate limit period (Gemini: 15 RPM)\n",
        "      time.sleep(5)\n",
        "\n",
        "    return responses_dict, raw_responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5hkSlAP7wgA"
      },
      "source": [
        "## 6. Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB4Xq4gl607U"
      },
      "source": [
        "In order to deal with the Gemini free API key contraints, we extract a random subportion of the entire development set on which predictions are made"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OwCYLk62kb9"
      },
      "outputs": [],
      "source": [
        "N = 200\n",
        "\n",
        "with open('/content/EXIST2024_dev.json', 'r') as file:\n",
        "    dataset = json.load(file)\n",
        "\n",
        "random_subset = dict(random.sample(list(dataset.items()), N))\n",
        "\n",
        "with open(\"EXIST2024_dev_subset.json\", \"w\") as json_file:\n",
        "    json.dump(random_subset, json_file, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRyEMqB520Ic"
      },
      "source": [
        "- task: Select the task (1, 2 or 3)\n",
        "- eval: Evaluation type ('soft' or 'hard')\n",
        "- prompt: If you would like to change the prompt from a text file instead of in the code.\n",
        "- dataframe: Data Frame created to pass through in the section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhyIhOuWh677"
      },
      "outputs": [],
      "source": [
        "json_location = '/content/EXIST2024_dev_subset.json' # Test model\n",
        "test_case = \"EXIST2024\"\n",
        "df = json_df(json_location, 'df')\n",
        "test_params = {\n",
        "    'task': 3,\n",
        "    'eval': 'soft',\n",
        "    'prompt': 'Prompts/prompt.txt',\n",
        "    'dataframe': df\n",
        "    }\n",
        "base_filename = f\"task{test_params['task']}_{test_params['eval']}\"\n",
        "\n",
        "responses, raw_responses = main(test_params)\n",
        "\n",
        " # Saving the normalized responses if they exist\n",
        "if responses:\n",
        "  save_responses(responses, output_dir='test_formats/gemini-pro/test', base_filename=base_filename)\n",
        "\n",
        "# Optionally save the raw responses too\n",
        "if raw_responses:\n",
        "  save_responses(raw_responses, output_dir='raw_formats/gemini-pro/test', base_filename=base_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrU5i_QhJkxf"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0jq6EdL7YOA"
      },
      "outputs": [],
      "source": [
        "# Gets the ground truths for the tweets in the test subset\n",
        "golds_path = f\"/content/EXIST2024_dev_task{test_params['task']}_gold_{test_params['eval']}.json\"\n",
        "\n",
        "with open('/content/EXIST2024_dev_subset.json', 'r') as j:\n",
        "     data = json.loads(j.read())\n",
        "choosen_ids = list(data.keys())\n",
        "\n",
        "with open(golds_path, 'r') as j:\n",
        "     data = json.loads(j.read())\n",
        "\n",
        "result = []\n",
        "\n",
        "for el in data:\n",
        "  if el['id'] in choosen_ids:\n",
        "    result.append(el)\n",
        "\n",
        "with open(f\"/content/EXIST2024_dev_task{test_params['task']}_gold_{test_params['eval']}_subset.json\", 'w') as file:\n",
        "    json.dump(result, file, indent=4)\n",
        "\n",
        "# Path to the prediction file\n",
        "file_path = f\"/content/test_formats/gemini-pro/test/task{test_params['task']}_{test_params['eval']}_1.json\"\n",
        "# Sort the predictions dictionary by increasing value of id in order to allign preds and golds\n",
        "with open(file_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    data = sorted(data, key=lambda x: int(x['id']))\n",
        "\n",
        "# Transform the data\n",
        "if test_params['eval'] == 'hard' and test_params['task'] != 3:\n",
        "  for entry in data:\n",
        "    if isinstance(entry['value'], list):\n",
        "        entry['value'] = entry['value'][0]  # Replace the list with its single element\n",
        "\n",
        "# Overwrite the original file with the transformed data\n",
        "with open(file_path, 'w') as file:\n",
        "    json.dump(data, file, indent=4)\n",
        "\n",
        "with open(file_path, 'r') as j:\n",
        "     data = json.loads(j.read())\n",
        "     reordered_data = [\n",
        "    {\"test_case\": d[\"test_case\"], \"id\": d[\"id\"], \"value\": d[\"value\"]}\n",
        "    for d in data\n",
        "]\n",
        "\n",
        "with open(file_path, 'w') as file:\n",
        "    json.dump(reordered_data, file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNwY2SjMB7-B"
      },
      "outputs": [],
      "source": [
        "test = PyEvALLEvaluation()\n",
        "metrics=[]\n",
        "\n",
        "if test_params['task'] == 1:\n",
        "   if test_params['eval'] == 'soft':\n",
        "    metrics = [MetricFactory.ICMSoft.value, MetricFactory.ICMSoftNorm.value, MetricFactory.CrossEntropy.value]\n",
        "   elif test_params['eval'] == 'hard':\n",
        "    metrics = [MetricFactory.ICM.value, MetricFactory.ICMNorm.value, MetricFactory.FMeasure.value]\n",
        "elif test_params['task'] == 2:\n",
        "   if test_params['eval'] == 'soft':\n",
        "    metrics = [MetricFactory.ICMSoft.value, MetricFactory.ICMSoftNorm.value, MetricFactory.CrossEntropy.value]\n",
        "   elif test_params['eval'] == 'hard':\n",
        "    metrics = [MetricFactory.ICM.value, MetricFactory.ICMNorm.value, MetricFactory.FMeasure.value]\n",
        "elif test_params['task'] == 3:\n",
        "   if test_params['eval'] == 'soft':\n",
        "    metrics = [MetricFactory.ICMSoft.value, MetricFactory.ICMSoftNorm.value]\n",
        "   elif test_params['eval'] == 'hard':\n",
        "    metrics = [MetricFactory.ICM.value, MetricFactory.ICMNorm.value, MetricFactory.FMeasure.value]\n",
        "\n",
        "predictions = f\"/content/test_formats/gemini-pro/test/task{test_params['task']}_{test_params['eval']}_1.json\"\n",
        "gold = f\"/content/EXIST2024_dev_task{test_params['task']}_gold_{test_params['eval']}_subset.json\"\n",
        "\n",
        "params= dict()\n",
        "if test_params['task'] == 2:\n",
        "  params[PyEvALLUtils.PARAM_HIERARCHY] = {\"YES\":[\"DIRECT\",\"REPORTED\",\"JUDGEMENTAL\"], \"NO\":[]}\n",
        "elif test_params['task'] == 3:\n",
        "  params[PyEvALLUtils.PARAM_HIERARCHY] = {\"YES\":[\"IDEOLOGICAL-INEQUALITY\",\"STEREOTYPING-DOMINANCE\",\"OBJECTIFICATION\", \"SEXUAL-VIOLENCE\", \"MISOGYNY-NON-SEXUAL-VIOLENCE\"], \"NO\":[]}\n",
        "\n",
        "report = test.evaluate(predictions, gold, metrics, **params)\n",
        "report.print_report()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
